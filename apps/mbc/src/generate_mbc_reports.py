#!/usr/bin/env python3
"""
Merit Badge Counselor (MBC) Pipeline Manager
Orchestrates complete data pipeline from scraping to report generation
Designed for weekly execution with robust error handling and recovery

Based on patterns from beascout repository weekly pipeline
"""

import argparse
import json
import logging
import re
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

class PipelineStage:
    """Represents a single pipeline stage with status tracking"""

    def __init__(self, name: str, description: str, script_path: str = None,
                 required_files: List[str] = None, output_files: List[str] = None):
        self.name = name
        self.description = description
        self.script_path = script_path
        self.required_files = required_files or []
        self.output_files = output_files or []
        self.status = "pending"  # pending, running, completed, failed, skipped
        self.start_time = None
        self.end_time = None
        self.error_message = None

    def start(self):
        """Mark stage as running"""
        self.status = "running"
        self.start_time = datetime.now()

    def complete(self):
        """Mark stage as completed"""
        self.status = "completed"
        self.end_time = datetime.now()

    def fail(self, error_message: str):
        """Mark stage as failed with error"""
        self.status = "failed"
        self.end_time = datetime.now()
        self.error_message = error_message

    def skip(self, reason: str):
        """Mark stage as skipped"""
        self.status = "skipped"
        self.error_message = reason

    @property
    def duration(self) -> Optional[float]:
        """Get stage duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None

class MBCPipeline:
    """
    Main pipeline manager for Merit Badge Counselor reports
    Handles orchestration, error recovery, and status tracking
    """

    def __init__(self, skip_scraping: bool = False, google_drive: bool = False):
        self.skip_scraping = skip_scraping
        self.google_drive = google_drive

        self.start_time = datetime.now()
        self.session_id = self.start_time.strftime("%Y%m%d_%H%M%S")

        # Track file paths generated by pipeline stages
        self.scraped_session_dir = None
        self.mbc_data_file = None
        self.roster_join_file = None
        self.latest_report_dir = None

        # Setup logging
        self.setup_logging()

        # Define pipeline stages
        self.stages = self._define_pipeline_stages()

        # Initialize tracked files for existing data (when skipping stages)
        self._initialize_existing_files()

        # Status tracking
        self.status_file = Path("data/logs") / f"mbc_pipeline_status_{self.session_id}.json"
        self.status_file.parent.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"ğŸš€ MBC Pipeline initialized (Session: {self.session_id})")

    def setup_logging(self):
        """Configure logging with single timestamped file and console output"""
        log_dir = Path("data/logs")
        log_dir.mkdir(parents=True, exist_ok=True)

        log_file = log_dir / f"generate_mbc_reports_{self.session_id}.log"

        # Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )

        # Setup logger
        self.logger = logging.getLogger('mbc_pipeline')
        self.logger.setLevel(logging.INFO)

        # Clear any existing handlers
        self.logger.handlers.clear()

        # File handler
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

        self.logger.info(f"ğŸ“ Logging to: {log_file}")

    def _define_pipeline_stages(self) -> Dict[str, PipelineStage]:
        """Define all pipeline stages with dependencies"""
        stages = {}

        if not self.skip_scraping:
            stages["scraping"] = PipelineStage(
                name="scraping",
                description="Scrape Merit Badge Counselor data from ScoutBook",
                script_path="src/merit_badge_counselor_scraper.py",
                required_files=[],
                output_files=[
                    "data/processed/mbc_counselors.json"
                ]
            )

        stages["processing"] = PipelineStage(
            name="processing",
            description="Process rosters and join with MBC data (auto-detect latest rosters)",
            script_path="src/roster_processor.py",
            required_files=[
                "data/processed/mbc_counselors.json",
                "data/input/rosters/*.html"
            ],
            output_files=[
                "data/processed/roster_mbc_join.json"
            ]
        )

        stages["scout_demand"] = PipelineStage(
            name="scout_demand",
            description="Process Scout merit badge requests and demand analysis",
            script_path="src/scout_demand_processor.py",
            required_files=[
                "data/input/*.csv",
                "data/input/*.xlsx"
            ],
            output_files=[
                "data/processed/scout_demand_analysis_*.json"
            ]
        )

        stages["coverage_analysis"] = PipelineStage(
            name="coverage_analysis",
            description="Analyze coverage gaps and generate priority rankings",
            script_path="src/coverage_gap_analyzer.py",
            required_files=[
                "data/processed/scout_demand_analysis_*.json",
                "data/processed/roster_mbc_join.json"
            ],
            output_files=[
                "data/processed/coverage_priority_analysis_*.json"
            ]
        )

        stages["reporting"] = PipelineStage(
            name="reporting",
            description="Generate HTML and PDF reports",
            script_path="src/report_generator.py",
            required_files=[
                "data/processed/roster_mbc_join.json",
                "data/processed/coverage_priority_analysis_*.json"
            ],
            output_files=[
                "data/reports/*/*.html",
                "data/reports/*/*.pdf"
            ]
        )

        if self.google_drive:
            stages["gdrive_prep"] = PipelineStage(
                name="gdrive_prep",
                description="Prepare files for Google Drive upload",
                script_path="src/prepare_gdrive_files.py",
                required_files=[
                    "data/reports/*/*.pdf"
                ],
                output_files=[
                    "data/gdrive/*.pdf"
                ]
            )

        return stages

    def _initialize_existing_files(self):
        """Initialize tracked file paths for existing data when skipping stages"""
        # Check for existing MBC data
        mbc_file = Path("data/processed/mbc_counselors.json")
        if mbc_file.exists():
            self.mbc_data_file = str(mbc_file)
            self.logger.info(f"ğŸ“„ Found existing MBC data: {self.mbc_data_file}")

        # Check for existing roster join data
        default_roster_file = Path("data/processed/roster_mbc_join.json")
        if default_roster_file.exists():
            self.roster_join_file = str(default_roster_file)
            self.logger.info(f"ğŸ“„ Found existing roster data: {self.roster_join_file}")

    def _extract_timestamp_from_filename(self, file_path: Path) -> Optional[datetime]:
        """Extract timestamp from filename patterns, not filesystem"""
        filename = file_path.name

        # Common patterns in MBC files
        patterns = [
            r'(\d{8}_\d{6})',      # YYYYMMDD_HHMMSS
            r'(\d{2}[A-Za-z]{3}\d{4})',  # 21Sep2025
            r'(\d{4}-\d{2}-\d{2})',      # 2025-09-21
        ]

        formats = [
            '%Y%m%d_%H%M%S',
            '%d%b%Y',
            '%Y-%m-%d'
        ]

        for pattern, fmt in zip(patterns, formats):
            match = re.search(pattern, filename)
            if match:
                try:
                    return datetime.strptime(match.group(1), fmt)
                except ValueError:
                    continue

        return None

    def check_dependencies(self, stage: PipelineStage) -> bool:
        """Check if all required files exist for a stage"""
        missing_files = []

        for required_file in stage.required_files:
            # Handle glob patterns
            if '*' in required_file:
                matches = list(Path(".").glob(required_file))
                if not matches:
                    missing_files.append(required_file)
                else:
                    self.logger.info(f"âœ… Found {len(matches)} files matching {required_file}")
            else:
                if not Path(required_file).exists():
                    missing_files.append(required_file)
                else:
                    self.logger.info(f"âœ… Found required file: {required_file}")

        if missing_files:
            self.logger.error(f"âŒ Missing required files for {stage.name}: {missing_files}")
            return False

        return True

    def find_latest_report_directory(self) -> Optional[Path]:
        """Find latest report directory using filename timestamps"""
        reports_dir = Path("data/reports")
        if not reports_dir.exists():
            return None

        latest_dir = None
        latest_timestamp = None

        # Pattern to match: *_MBC_Reports_YYYYMMDD_HHMMSS
        pattern = re.compile(r'.*_MBC_Reports_(\d{8}_\d{6})$')

        for item in reports_dir.iterdir():
            if item.is_dir():
                match = pattern.match(item.name)
                if match:
                    timestamp_str = match.group(1)
                    try:
                        timestamp = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                        if latest_timestamp is None or timestamp > latest_timestamp:
                            latest_timestamp = timestamp
                            latest_dir = item
                    except ValueError:
                        continue

        if latest_dir:
            self.logger.info(f"ğŸ“ Found latest report directory: {latest_dir.name}")
        else:
            self.logger.warning("âš ï¸ No MBC report directories found")

        return latest_dir

    def update_tracked_files(self, stage_name: str):
        """Update tracked file paths after stage completion"""
        if stage_name == "scraping":
            # Track MBC data file and scraped session
            self.mbc_data_file = "data/processed/mbc_counselors.json"
            # Find the most recent scraped session directory
            scraped_dirs = list(Path("data/scraped").glob("*"))
            if scraped_dirs:
                self.scraped_session_dir = max(scraped_dirs, key=lambda p: p.name)
                self.logger.info(f"ğŸ“ Tracked scraped session: {self.scraped_session_dir}")

        elif stage_name == "processing":
            # Track roster join file
            self.roster_join_file = "data/processed/roster_mbc_join.json"

        elif stage_name == "reporting":
            # Track latest report directory
            self.latest_report_dir = self.find_latest_report_directory()
            if self.latest_report_dir:
                self.logger.info(f"ğŸ“ Tracked report directory: {self.latest_report_dir}")

    def execute_stage(self, stage_name: str) -> bool:
        """Execute a single pipeline stage"""
        stage = self.stages[stage_name]

        self.logger.info(f"ğŸ”„ Starting stage: {stage.name} - {stage.description}")
        stage.start()

        try:
            # Check dependencies
            if not self.check_dependencies(stage):
                stage.fail("Missing required files")
                return False

            # Build command with explicit file arguments
            cmd = [sys.executable, stage.script_path]

            # Stage-specific arguments using tracked file paths
            if stage_name == "processing":
                # Use explicit MBC data file if available
                if self.mbc_data_file and Path(self.mbc_data_file).exists():
                    cmd.extend(["--mbc-data", self.mbc_data_file])
                    self.logger.info(f"ğŸ“„ Using MBC data: {self.mbc_data_file}")

            elif stage_name == "reporting":
                # Use explicit roster join file
                if self.roster_join_file and Path(self.roster_join_file).exists():
                    cmd.extend(["--data-file", self.roster_join_file])
                    self.logger.info(f"ğŸ“„ Using roster data: {self.roster_join_file}")

            elif stage_name == "gdrive_prep":
                # Use tracked latest report directory
                if self.latest_report_dir:
                    cmd.extend(["--reports-dir", str(self.latest_report_dir.parent)])
                else:
                    # Fallback to auto-detection
                    latest_report = self.find_latest_report_directory()
                    if latest_report:
                        cmd.extend(["--reports-dir", str(latest_report.parent)])

            # Execute command with real-time logging
            self.logger.info(f"ğŸš€ Executing: {' '.join(cmd)}")

            # Use Popen for real-time output capture
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,  # Merge stderr into stdout
                text=True,
                bufsize=1,  # Line buffered
                universal_newlines=True,
                cwd=Path.cwd()
            )

            # Log output in real-time
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # Forward to pipeline logger
                    self.logger.info(f"  {output.strip()}")

            # Get final return code
            return_code = process.poll()

            # Check result
            if return_code != 0:
                stage.fail(f"Script failed with return code {return_code}")
                return False

            stage.complete()
            duration = stage.duration
            self.logger.info(f"âœ… Completed stage: {stage.name} ({duration:.1f}s)")

            # Track output files for next stages
            self.update_tracked_files(stage_name)

            return True

        except Exception as e:
            stage.fail(str(e))
            self.logger.error(f"âŒ Stage {stage.name} failed: {e}")
            return False

    def save_status(self):
        """Save current pipeline status to JSON file"""
        status = {
            "session_id": self.session_id,
            "start_time": self.start_time.isoformat(),
            "skip_scraping": self.skip_scraping,
            "google_drive": self.google_drive,
            "stages": {}
        }

        for stage_name, stage in self.stages.items():
            status["stages"][stage_name] = {
                "name": stage.name,
                "description": stage.description,
                "status": stage.status,
                "start_time": stage.start_time.isoformat() if stage.start_time else None,
                "end_time": stage.end_time.isoformat() if stage.end_time else None,
                "duration": stage.duration,
                "error_message": stage.error_message
            }

        with open(self.status_file, 'w') as f:
            json.dump(status, f, indent=2)

    def run_pipeline(self, stage_filter: str = "all") -> bool:
        """Run the complete pipeline or specific stages"""
        self.logger.info(f"ğŸ¯ Running MBC Pipeline - Stage filter: {stage_filter}")
        self.logger.info(f"ğŸ“Š Configuration: skip_scraping={self.skip_scraping}, google_drive={self.google_drive}")

        # Determine stages to run
        if stage_filter == "all":
            stages_to_run = list(self.stages.keys())
        else:
            stages_to_run = [stage_filter] if stage_filter in self.stages else []

        if not stages_to_run:
            self.logger.error(f"âŒ Invalid stage: {stage_filter}")
            return False

        self.logger.info(f"ğŸ“‹ Stages to run: {', '.join(stages_to_run)}")

        success = True

        for stage_name in stages_to_run:
            self.save_status()

            if not self.execute_stage(stage_name):
                success = False
                break

        # Final status save
        self.save_status()

        # Report results
        total_time = (datetime.now() - self.start_time).total_seconds()

        if success:
            self.logger.info(f"ğŸ‰ Pipeline completed successfully! (Total time: {total_time:.1f}s)")
            self.print_summary()
        else:
            self.logger.error(f"âŒ Pipeline failed after {total_time:.1f}s")

        return success

    def print_summary(self):
        """Print summary of pipeline execution"""
        self.logger.info("\nğŸ“Š Pipeline Summary:")
        self.logger.info("=" * 60)

        for stage_name, stage in self.stages.items():
            status_icon = {
                "completed": "âœ…",
                "failed": "âŒ",
                "skipped": "â­ï¸",
                "pending": "â¸ï¸"
            }.get(stage.status, "â“")

            duration_str = f"({stage.duration:.1f}s)" if stage.duration else ""
            self.logger.info(f"{status_icon} {stage.name}: {stage.status} {duration_str}")

            if stage.error_message:
                self.logger.info(f"    Error: {stage.error_message}")

        self.logger.info("=" * 60)

        # Show key outputs
        self.logger.info("ğŸ“ Key Files Generated:")
        if Path("data/processed/roster_mbc_join.json").exists():
            self.logger.info("  ğŸ“„ data/processed/roster_mbc_join.json")

        # Check for scout demand analysis files
        demand_files = sorted(Path("data/processed").glob("scout_demand_analysis_*.json"))
        if demand_files:
            latest_demand = demand_files[-1]
            self.logger.info(f"  ğŸ“Š {latest_demand}")

        # Check for coverage priority analysis files
        priority_files = sorted(Path("data/processed").glob("coverage_priority_analysis_*.json"))
        if priority_files:
            latest_priority = priority_files[-1]
            self.logger.info(f"  ğŸ¯ {latest_priority}")

        latest_report = self.find_latest_report_directory()
        if latest_report:
            self.logger.info(f"  ğŸ“‹ {latest_report}")

        if self.google_drive and Path("data/gdrive").exists():
            gdrive_files = list(Path("data/gdrive").glob("*.pdf"))
            if gdrive_files:
                self.logger.info(f"  ğŸ“¤ data/gdrive/ ({len(gdrive_files)} PDF files ready for upload)")

        self.logger.info(f"ğŸ“ Status file: {self.status_file}")
        self.logger.info(f"ğŸ“ Log file: data/logs/generate_mbc_reports_{self.session_id}.log")

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Merit Badge Counselor Pipeline Manager",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Pipeline Stages:
  scraping         - Scrape Merit Badge Counselor data from ScoutBook
  processing       - Process rosters and join with MBC data
  scout_demand     - Process Scout merit badge requests and demand analysis
  coverage_analysis - Analyze coverage gaps and generate priority rankings
  reporting        - Generate HTML and PDF reports (includes priority analysis)
  gdrive_prep      - Prepare files for Google Drive upload (requires --google-drive)

Examples:
  python generate_mbc_reports.py                          # Full pipeline with auto-detection
  python generate_mbc_reports.py --skip-scraping         # Skip scraping (use existing data)
  python generate_mbc_reports.py --google-drive          # Include Google Drive preparation
  python generate_mbc_reports.py --stage processing      # Run specific stage
  python generate_mbc_reports.py --stage scout_demand    # Process Scout requests only
  python generate_mbc_reports.py --stage reporting       # Generate reports only
        """
    )

    parser.add_argument(
        '--stage',
        choices=['scraping', 'processing', 'scout_demand', 'coverage_analysis', 'reporting', 'gdrive_prep', 'all'],
        default='all',
        help='Pipeline stage to run [default: all]'
    )

    parser.add_argument(
        '--skip-scraping',
        help='Skip scraping stage and use existing MBC data',
        action='store_true'
    )

    parser.add_argument(
        '--google-drive',
        help='Include Google Drive file preparation stage',
        action='store_true'
    )

    return parser.parse_args()

def main():
    """Main entry point"""
    args = parse_arguments()

    try:
        # Initialize pipeline
        pipeline = MBCPipeline(
            skip_scraping=args.skip_scraping,
            google_drive=args.google_drive
        )

        # Run pipeline
        success = pipeline.run_pipeline(stage_filter=args.stage)

        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\nâš ï¸ Pipeline cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Pipeline failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()